{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제1 고지 : 미분 자동 계산 \n",
    "## STEP 8 : 재귀에서 반복문으로\n",
    "- 복잡한 계산 그래프를 다루는데, 재귀적 역전파 계산은 효율성이 떨어진다.  \n",
    "### 8.1 현재의 Variable 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "class Variable:\n",
    "    def __init__(self, data: np.ndarray) -> None:\n",
    "        self.data = data\n",
    "        self.grad = None  # gradient\n",
    "        self.creator = None  # creator\n",
    "\n",
    "    def set_creator(self, func) -> None:\n",
    "        self.creator = func\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        자동 역전파 (재귀)\n",
    "        \"\"\"\n",
    "        f = self.creator  # 1. 함수를 가져온다\n",
    "        if f is not None:\n",
    "            x = f.input  # 2. 함수의 입력을 가져온다\n",
    "            x.grad = f.backward(self.grad)  # 3. 역전파를 계산한다\n",
    "            x.backward()  # 하나 앞 변수의 backward 메서드를 호출한다 (재귀)\n",
    "        # NOTE : 만약 creator가 None 이면 역전파가 중단된다. creator가 없으므로 해당 Variable 인스턴스는 함수 바깥에서 생성됐음을 뜻한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 반복문을 이용한 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "class Variable:\n",
    "    def __init__(self, data: np.ndarray) -> None:\n",
    "        self.data = data\n",
    "        self.grad = None  # gradient\n",
    "        self.creator = None  # creator\n",
    "\n",
    "    def set_creator(self, func) -> None:\n",
    "        self.creator = func\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        자동 역전파 (반복)\n",
    "        \"\"\"\n",
    "        funcs = [self.creator]\n",
    "        while funcs:\n",
    "            f = funcs.pop()  # 1. 함수를 가져온다\n",
    "            x, y = f.input, f.output  # 2. 함수의 입력 / 출력을 가져온다\n",
    "            x.grad = f.backward(y.grad)  # 3. 역전파를 계산한다\n",
    "\n",
    "            if x.creator is not None:\n",
    "                funcs.append(x.creator)  # 하나 앞의 함수를 리스트에 추가한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 동작 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자동 역전파 : 3.297442541400256\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Variable:\n",
    "    def __init__(self, data: np.ndarray) -> None:\n",
    "        self.data = data\n",
    "        self.grad = None  # gradient\n",
    "        self.creator = None  # creator\n",
    "\n",
    "    def set_creator(self, func) -> None:\n",
    "        self.creator = func\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        자동 역전파 (반복)\n",
    "        \"\"\"\n",
    "        funcs = [self.creator]\n",
    "        while funcs:\n",
    "            f = funcs.pop()  # 1. 함수를 가져온다\n",
    "            x, y = f.input, f.output  # 2. 함수의 입력 / 출력을 가져온다\n",
    "            x.grad = f.backward(y.grad)  # 3. 역전파를 계산한다\n",
    "\n",
    "            if x.creator is not None:\n",
    "                funcs.append(x.creator)  # 하나 앞의 함수를 리스트에 추가한다.\n",
    "\n",
    "\n",
    "class Function:\n",
    "    \"\"\"\n",
    "    Function Base Class\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, input: Variable) -> Variable:\n",
    "        x = input.data\n",
    "        y = self.forward(x)\n",
    "        self.input = input  # 역전파 계산을 위해 입력변수 보관\n",
    "        output = Variable(y)\n",
    "        output.set_creator(self)  # 출력 변수에 creator 설정 ( 연결을 동적으로 만드는 핵심)\n",
    "        self.output = output  # 출력도 저장\n",
    "\n",
    "        return output\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        구체적인 함수 계산 담당\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward(self, gy: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        역전파\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class Square(Function):\n",
    "    \"\"\"\n",
    "    y= x ^ 2\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        return x**2\n",
    "\n",
    "    def backward(self, gy: np.ndarray) -> np.ndarray:\n",
    "        x = self.input.data\n",
    "        gx = 2 * x * gy\n",
    "        return gx\n",
    "\n",
    "\n",
    "class Exp(Function):\n",
    "    \"\"\"\n",
    "    y=e ^ x\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.exp(x)\n",
    "\n",
    "    def backward(self, gy: np.ndarray) -> np.ndarray:\n",
    "        x = self.input.data\n",
    "        gx = np.exp(x) * gy\n",
    "        return gx\n",
    "\n",
    "\n",
    "class Sigmoid(Function):\n",
    "    \"\"\"\n",
    "    y = 1 / (1 + e ^(-x))\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def backward(self, gy: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        d/dx sigmoid(x) = sigmoid(x)(1-sigmoid(x))\n",
    "        \"\"\"\n",
    "        x = self.input.data\n",
    "        sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "        return gy * sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "class Tanh(Function):\n",
    "    \"\"\"\n",
    "    y= ( e^x - e^{-x} ) / ( e^x + e^{-x} )\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "    def backward(self, gy: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        d/dx tanh(x) = 1-tanh(x)^2\n",
    "        \"\"\"\n",
    "        x = self.input.data\n",
    "        tanh = lambda x: (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "        return gy * (1 - tanh(x) ** 2)\n",
    "\n",
    "\n",
    "x = Variable(np.array(0.5))\n",
    "A = Square()\n",
    "B = Exp()\n",
    "C = Square()\n",
    "\n",
    "a = A(x)\n",
    "b = B(a)\n",
    "y = C(b)\n",
    "\n",
    "## 자동 역전파\n",
    "y.grad = np.array(1.0)\n",
    "y.backward()\n",
    "print(f\"자동 역전파 : {x.grad}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dezero : 0.09112821805819912\n",
      "PyTorch : tensor([0.0911])\n"
     ]
    }
   ],
   "source": [
    "# Dezero ~ Pytorch\n",
    "## Dezero\n",
    "x = Variable(np.array(1.0))\n",
    "A = Tanh()\n",
    "B = Sigmoid()\n",
    "a = A(x)\n",
    "b = B(a)\n",
    "\n",
    "b.grad = np.array(1.0)\n",
    "b.backward()\n",
    "print(f\"Dezero : {x.grad}\")\n",
    "\n",
    "## Pytorch\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "A = nn.Tanh()\n",
    "B = nn.Sigmoid()\n",
    "a = A(x)\n",
    "b = B(a)\n",
    "b.backward() \n",
    "print(f\"PyTorch : {x.grad}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3584091cda227b8e59fda59e5fdf3aec4997f3a2464c55243d7618073e2ad776"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
