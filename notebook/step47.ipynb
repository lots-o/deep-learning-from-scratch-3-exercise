{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제4 고지 : 신경망 만들기 \n",
    "## STEP 47 : 소프트맥스 함수와 교차 엔트로피 오차\n",
    "\n",
    "이전 단계에서는 회귀문제를 풀어봤는데, 앞으로는 `multi-class classification` 에 대해 알아본다.\n",
    "\n",
    "\n",
    "### 47.1 슬라이스 조작 함수\n",
    "\n",
    "구체적인 구현은 추후 부록.B를 참고하도록 하고, 사용법에 대해서만 우선 살펴본다.\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='../assets/그림 47-1.png' align='center' width='50%'>\n",
    "</p>\n",
    "\n",
    "`get_item()` 은 `Variable` 의 다차원 배열 중에서 일부를 슬라이스 하여 뽑아준다. 즉 **슬라이스는 데이터의 일부를 수정하지 않고 그대로 전달하는 역할**이므로, **데이터가 추출된 위치에만 기울기를 1로 설정하고 이외에는 0으로 설정**한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슬라이스 :  variable([4 5 6])\n",
      "==========\n",
      "기울기 :  variable([[0 0 0]\n",
      "          [1 1 1]])\n",
      "==========\n",
      "variable([[1 2 3]\n",
      "          [1 2 3]\n",
      "          [4 5 6]])\n",
      "==========\n",
      "variable([4 5 6])\n",
      "variable([3 6])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np \n",
    "from dezero import Variable\n",
    "import dezero.functions as F \n",
    "\n",
    "x = Variable(np.array([[1,2,3],[4,5,6]]))\n",
    "y = F.get_item(x,1)\n",
    "print(\"슬라이스 : \", y)\n",
    "print(\"=\"*10)\n",
    "y.backward()\n",
    "print(\"기울기 : \",x.grad)\n",
    "print(\"=\"*10)\n",
    "\n",
    "# 또한 같은 인덱스를 반복 지정하여 동일한 원소를 여러 번 빼낼 수 있다.\n",
    "indices = np.array([0,0,1])\n",
    "y = F.get_item(x,indices)\n",
    "print(y)\n",
    "\n",
    "print(\"=\"*10)\n",
    "# 또한 dezero/core.py 에서 Variable.__getitem__ = dezero.functions.get_item 로 다음과 같이 슬라이스 연산이 가능하며 역전파 역시 잘 작동한다.\n",
    "y = x[1]\n",
    "print(y)\n",
    "\n",
    "y = x[:,2]\n",
    "print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 47.2 소프트맥스 함수\n",
    "신경망의 출력은 단순한 수치인데, 다중 클래스 분류를 위해 이 수치를 '확률'로 변환할 필요가 있다.  \n",
    "다음 softmax function을 이용하면 확률로 변환 가능하다. \n",
    "\n",
    "$$\n",
    "p_k = \\frac{e^{y_k}}{\\sum_{i=1}^n e^{y_i}}\n",
    "$$\n",
    "\n",
    "구체적으로 살펴보면, 합수의 입력 $y_k$  총 $n$ 개 (클래스 수) 라고 가정하면 $p_1+p_2+\\cdots+p_n = 1 $ 이 성립하여 $(p_1,p_2,\\cdots,p_n)$의 원소 각각을 확률로 해석할 수 있다.\n",
    "\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='../assets/그림 47-2.png' align='center' width='70%'>\n",
    "</p>\n",
    "\n",
    "이해를 돕기 위해 위의 그림을 보면 소프트맥스 함수를 적용하고나면 확률 $p_1,p_2,p_3$로 표현되고 합이 1 임을 알 수 있다.\n",
    "\n",
    "<span style='background-color : #ffdce0'>💡<b>주의해야할 것은 소프트맥스 함수는 지수함수로 이루어져 있기 때문에 너무 커지거나 작아질 수 있다. 따라서 오버플로우 문제를 잘 다뤄줘야 하는데 실제 구현시에는 $p_k = \\frac{Ce^{y_k}}{C\\sum_{i=1}^n e^{y_i}}=\\frac{e^{y_k}+\\log C}{\\sum_{i=1}^n e^{y_i}+\\log C}=\\frac{e^{y_k}+C'}{\\sum_{i=1}^n e^{y_i}+C'}$와 같이   지수함수계산시에는 어떤 정수를 더하거나 빼더라도 결과는 바뀌지 않는 다는 사실을 이용해 최대값을 빼서 오버플로우를 막아준다.</b></span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신경망 출력 :  variable([[ 0.12806286 -0.08324128  0.3620511 ]])\n",
      "소프트 맥스 적용 후 :  variable([[0.32539823 0.26341892 0.41118285]])\n"
     ]
    }
   ],
   "source": [
    "from dezero import Variable, as_variable\n",
    "from dezero.models import MLP\n",
    "import dezero.functions as F \n",
    "\n",
    "def softmax1d(x):\n",
    "    x = as_variable(x)\n",
    "    y = F.exp(x)\n",
    "    sum_y = F.sum(y)\n",
    "    return y / sum_y\n",
    "\n",
    "x = Variable(np.array([[0.2,-0.4]]))\n",
    "model = MLP((10,3))\n",
    "y = model(x)\n",
    "p = softmax1d(y)\n",
    "print(\"신경망 출력 : \",y)\n",
    "print(\"소프트 맥스 적용 후 : \",p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이어서 **배치 데이터에서도 적용 가능**하게 하려면 다음과 같이 함수를 수정한다.\n",
    "```python\n",
    "def softmax_simple(x,axis=1):\n",
    "    x = as_variable(x)\n",
    "    y = exp(x) # DeZero exp 함수\n",
    "    sum_y = sum(y,axis=axis,keepdims=True)\n",
    "    return y / sum_y \n",
    "```\n",
    "\n",
    "추가적으로, 더 나은 구현 방식을 위해 `Function`을 상속하여 `Softmax` 클래스를 구현해보자.\n",
    "```python\n",
    "class Softmax(Function):\n",
    "    def __init__(self, axis=1):\n",
    "        self.axis = axis\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x - x.max(axis=self.axis, keepdims=True) # 오버플로우 방지를 위해 최대값을 빼준다.\n",
    "        y = np.exp(y)\n",
    "        y /= y.sum(axis=self.axis, keepdims=True)\n",
    "        return y\n",
    "\n",
    "    def backward(self, gy):\n",
    "        y = self.outputs[0]()\n",
    "        gx = y * gy\n",
    "        sumdx = gx.sum(axis=self.axis, keepdims=True)\n",
    "        gx -= y * sumdx\n",
    "        return gx\n",
    "\n",
    "def softmax(x, axis=1):\n",
    "    return Softmax(axis)(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 47.3 교차 엔트로피 오차 \n",
    "선형회귀에서는 `MSE` 를 손실함수로 사용했다면, multi-class classification  에서는 `교차 엔트로피 오차 (cross entropy error)` 를 손실함수로 일반적으로 사용한다. \n",
    "\n",
    "$$\n",
    "L = - \\sum_k t_k\\log p_k\n",
    "$$\n",
    "\n",
    "여기서 $t_k$ 는 정답 데이터의 $k$차원의 값으로 정답이면 1, 아니면 0으로 표현되는 원핫 벡터로 실제분포이고,  $p_k$ 는 신경망에서 소프트맥스를 적용한 함수 값으로 예측 분포이다.\n",
    " \n",
    "\n",
    "이를 조금더 간단하게 표현하면 다음과 같다. \n",
    "$$\n",
    "L= -\\log\\mathbf{p}[t]\n",
    "$$\n",
    "\n",
    "구체적으로 살펴보면, 만약 $\\mathbf{t}=(0,0,1),\\mathbf{p}=(p_0,p_1,p_2)$라고 하면 $L=-\\log p_2$이므로 정답클래스에 해당하는 번호의 확률 $\\mathbf{p}$를 추출하는것과 같으므로, 위와 같이 표현이 가능하다.\n",
    "\n",
    "<span style='background-color : #ffdce0'>💡<b>이번 설명은 데이터가 하나인 경우에 해당하고 만약 데이터가 $N$개라면 평균 교차엔트로피 오차를 구해야 한다.</b></span>\n",
    "\n",
    "이제 `소프트맥스 함수` 와 `교차 엔트로피 오차` 를 한꺼번에 수행하는 `softmax_cross_entropy_simple(x,t)` 함수를 구현해보자.\n",
    "\n",
    "\n",
    "```python\n",
    "def softmax_cross_entropy_simple(x, t):\n",
    "    '''\n",
    "    x : 신경망에서 소프트맥수 적용전의 출력값\n",
    "    t : 정답 데이터로 정답 클래스의 번호 (원핫 벡터가 아니다)\n",
    "    '''\n",
    "    x, t = as_variable(x), as_variable(t)\n",
    "    N = x.shape[0]\n",
    "    p = softmax(x)\n",
    "    p = clip(p, 1e-15, 1.0)  # To avoid log(0), 만약 p가 0이면 1e-15로 대체, 1.0을 넘으면 1.0\n",
    "    log_p = log(p)\n",
    "    tlog_p = log_p[np.arange(N), t.data]\n",
    "    y = -1 * sum(tlog_p) / N\n",
    "    return y\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(0.9820149162942775)\n"
     ]
    }
   ],
   "source": [
    "from dezero import Variable, as_variable\n",
    "from dezero.models import MLP\n",
    "import dezero.functions as F \n",
    "\n",
    "\n",
    "x = np.array([[0.2,-0.4],[0.3,0.5],[1.3,-3.2],[2.1,0.3]])\n",
    "t = np.array([2,0,1,0])\n",
    "model = MLP((10,3))\n",
    "y = model(x)\n",
    "loss = F.softmax_cross_entropy(y,t)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3584091cda227b8e59fda59e5fdf3aec4997f3a2464c55243d7618073e2ad776"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
