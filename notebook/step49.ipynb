{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제4 고지 : 신경망 만들기 \n",
    "## STEP 49 : Dataset 클래스와 전처리\n",
    "\n",
    "이전에 spiral dataset을 가져올 때는 데이터의 갯수가 작았으므로, `ndarray` 하나로 표현할 수 있었지만, 대규모 데이터셋의 경우에는 모든 데이터를 메모리에 올릴 수 없다. 그래서 이를 대응하기 위해 `Dataset` 클래스를 만들어 데이터를 처리할 수 있도록 한다.\n",
    "\n",
    "\n",
    "### 49.1 Dataset 클래스 구현\n",
    "\n",
    "`Dataset` 클래스는 기반 클래스로서의 역할을 하고, 사용자가 실제로 사용하는 데이터셋은 이를 상속하여 구현한다.\n",
    "\n",
    "```python\n",
    "class Dataset:\n",
    "    def __init__(self,train=True):\n",
    "        self.train = train\n",
    "        self.data = None\n",
    "        self.label = None\n",
    "        self.prepare() \n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        assert np.isscalar(index) # index 는 스칼라(정수)만 가능\n",
    "        if self.label is None:\n",
    "            return self.data[index],None\n",
    "        else:\n",
    "            return self.data[index],self.label[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def prepare(self):\n",
    "        pass\n",
    "```\n",
    "\n",
    "여기서 주목해야 할것은 `__getitem__` 과 `__len__` 메서드로, `x[0]` 과 같이 인덱스로 접근가능하며 `len()` 함수를 호출하여 데이터셋의 크기를 알 수 있다는 것이다. \n",
    "\n",
    "이제 이를 상속하여 spiral dataset 을 구현하면 다음과 같다.\n",
    "\n",
    "```python\n",
    "class Spiral(Dataset):\n",
    "    def prepare(self):\n",
    "        self.data,self.label = get_spiral(self.train)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.13981389, -0.00721657], dtype=float32), 1)\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import dezero\n",
    "\n",
    "train_set = dezero.datasets.Spiral(train=True)\n",
    "print(train_set[0])\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 49.2 큰 데이터셋의 경우\n",
    "\n",
    "현재는 작은 데이터셋에는 문제 없지만, 데이터셋이 훨씬 크다면 이런 방식은 사용핦 수 없다. 그래서 다음과 같은 구현방식을 생각해본다.\n",
    "\n",
    "1. `data` `label` 디렉토리에 각각 100만개의 데이터가 저장 (ex, `data/0.npy`, `label/1.npy`)\n",
    "2. `Dataset` 클래스를 초기화할때 데이터를 불러오는 것이 아닌 데이터에 접근할 때 데이터를 읽게 하는 것이다.\n",
    "\n",
    "```python\n",
    "class BigData(dataset):\n",
    "    def __getitem__(index):\n",
    "        x = np.load(\"data/{}.npy\".format(index))\n",
    "        t = np.load(\"label/{}.npy\".format(index))\n",
    "        return \n",
    "```\n",
    "\n",
    "### 49.3 데이터 이어 붙이기\n",
    "\n",
    "신경망을 학습 시킬 때 데이터셋 중 일부를 미니배치로 가져온다. \n",
    "\n",
    "```python\n",
    "train_set = dezero.datasets.Spiral()\n",
    "batch_index = [0,1,2] \n",
    "batch = [train_set[i] for i in batch_index] \n",
    "# batch = [(data_0,label_0),(data_1,label_1),(data_2,label_2)]\n",
    "\n",
    "x = np.array([example[0] for example in batch])\n",
    "t = np.array([example[1] for example in batch])\n",
    "\n",
    "print(x.shape)\n",
    "# (3,2)\n",
    "print(t.shape)\n",
    "# (3,)\n",
    "```\n",
    "\n",
    "이와 같이 우선은 인덱스를 가져와 여러 데이터를 꺼내고 이 후 DeZero의 신경망에 입력하기 위해 다음과 같이 하나의 `ndarray` 인스턴스로 변환하여 이어붙인다.\n",
    "\n",
    "\n",
    "### 49.4 학습 코드\n",
    "그럼 이제 `Spiral` 클래스를 사용하여 학습을 진행해본다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.13\n",
      "epoch 21, loss 0.73\n",
      "epoch 41, loss 0.71\n",
      "epoch 61, loss 0.64\n",
      "epoch 81, loss 0.52\n",
      "epoch 101, loss 0.42\n",
      "epoch 121, loss 0.36\n",
      "epoch 141, loss 0.27\n",
      "epoch 161, loss 0.23\n",
      "epoch 181, loss 0.20\n",
      "epoch 201, loss 0.17\n",
      "epoch 221, loss 0.16\n",
      "epoch 241, loss 0.15\n",
      "epoch 261, loss 0.15\n",
      "epoch 281, loss 0.13\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import dezero\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "from dezero.models import MLP\n",
    "\n",
    "\n",
    "log_interval = 20 # 20 epoch 마다 logging\n",
    "\n",
    "# 1. 하이퍼 파라미터 설정\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "lr = 1.0\n",
    "\n",
    "\n",
    "# 2. 데이터 읽기 / 모델, 옵티마이저 생성\n",
    "######################################\n",
    "train_set = dezero.datasets.Spiral()\n",
    "######################################\n",
    "\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "######################################\n",
    "data_size = len(train_set)\n",
    "######################################\n",
    "\n",
    "max_iter = math.ceil(data_size / batch_size)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    # 3. 데이터셋 셔플\n",
    "    index = np.random.permutation(data_size)\n",
    "    sum_loss = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # 4. 미니 배치 생성\n",
    "        batch_index = index[i * batch_size:(i + 1) * batch_size]\n",
    "        #########################################################\n",
    "        batch = [train_set[i] for i in batch_index] \n",
    "        batch_x = np.array([example[0] for example in batch])\n",
    "        batch_t = np.array([example[1] for example in batch])\n",
    "        #########################################################\n",
    "        # 5. 기울기 산출 / 매개변수 갱신\n",
    "        y = model(batch_x)\n",
    "        loss = F.softmax_cross_entropy(y, batch_t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(batch_t)\n",
    "\n",
    "    # 6. 에폭마다 학습 경과 출력\n",
    "    avg_loss = sum_loss / data_size\n",
    "    if epoch % log_interval == 0:\n",
    "        print('epoch %d, loss %.2f' % (epoch + 1, avg_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 49.5 데이터 전처리\n",
    "\n",
    "머신러닝에서는 모델에 데이터를 입력하기 전에 데이터를 특정한 형태로 가공하는 일이 많다.  \n",
    "예를 들어 **데이터에서 특정 값을 제거하거나 형상을 변환**하는 것이다. 이미지 인식에서는 **데이터 증강(data augmentation)** 을 위해 자주 사용되기도 한다. 따라서 해당 기능을 추가해본다.\n",
    "\n",
    "```python\n",
    "class Dataset:\n",
    "    def __init__(self, train=True, transform=None, target_transform=None):\n",
    "        self.train = train\n",
    "        ##################################################\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        if self.transform is None:\n",
    "            self.transform = lambda x: x\n",
    "        if self.target_transform is None:\n",
    "            self.target_transform = lambda x: x\n",
    "        ##################################################\n",
    "\n",
    "        self.data = None\n",
    "        self.label = None\n",
    "        self.prepare()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert np.isscalar(index)\n",
    "        ##################################################\n",
    "        if self.label is None:\n",
    "            return self.transform(self.data[index]), None\n",
    "        else:\n",
    "            return self.transform(self.data[index]),\\\n",
    "                   self.target_transform(self.label[index])\n",
    "        ##################################################\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def prepare(self):\n",
    "        pass\n",
    "\n",
    "```\n",
    "\n",
    "구체적으로 살펴보면 `transform` 과 `target_transform` 은 각각 데이터, 레이블에 대핸 변환을 수행하는 `Callable` 객체이다. 만약 정의된 것이 없다면 데이터,레이블을 그대로 반환하도록 한다.\n",
    "\n",
    "<span style='background-color : #ffdce0'>💡<b>(`dezero/transforms.py` 전처리시 자주 사용되는 변환들이 구현되어 있다.) </b></span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero import transforms\n",
    "\n",
    "f = transforms.Normalize(mean=0.0,std=2.0)\n",
    "train_set = dezero.datasets.Spiral(transform=f)\n",
    "\n",
    "f = transforms.Compose([transforms.Normalize(mean=0.0,std=2.0),transforms.AsType(np.float64)])\n",
    "train_set = dezero.datasets.Spiral(transform=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3584091cda227b8e59fda59e5fdf3aec4997f3a2464c55243d7618073e2ad776"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
